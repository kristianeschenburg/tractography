#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Tue Jan  9 15:15:14 2018

@author: kristianeschenburg
"""

import json
import numpy as np
import pandas as pd
import scipy

def smoothFeatures(fdt,adjacency,depth,kernelWeight):
    
    """
    Method to "smear" streamline counts generated by probtrackx2.  For a given
    vertex, the vertex projects to (possibly) all other vertices on the surface.
    If we correlate these raw streamline counts, two vertices with very similar,
    but non-identical spatial distributions, will produce a correlation of 0, 
    even though their anatomical connections are similar.
    
    Here, we smear the projections to a single vertex amongst its neighbors,
    using a Gaussian Kernel.
    
    Parameters:
    - - - - -        
        fdt : raw connectivity matrix
        adjacency : adjacency graph
        depth : sulcal depth map (scaled and shifted, so that deepest
                sulcus has a depth of 0)
        kernelSize : neighborhood size
        kernelWeight : weight to apply to neighborhood
    """
    
    depth = depth + (-1.*np.min(depth))
    
    inputs = np.arange(fdt.shape[0])
    
    source = fdt['source']
    target = fdt['target']
    count = fdt['count']
    
    updates = np.zeros((len(depth),len(depth)))
    
    print len(inputs)
    
    for i in inputs:
        
        if i % 50000 == 0:
            print i
        
        s = source[i]-1
        t = target[i]-1
        c = count[i]
        
        try:
            neighbs = adjacency[s]
        except:
            print s
        dp = depth[s]
        
        results = smoothIndex(s,t,c,neighbs,dp,kernelWeight)
        
        updates[s,results.keys()] += results.values()
    
    """
    results = Parallel(n_jobs=NUM_CORES)(delayed(smoothIndex)(source[i],
                       target[i],count[i],adjacency[i],depth[source[i]],
                       kernelWeight,normalize=False) for i in inputs)
    """
    
    sparseUpdate = scipy.sparse.csr_matrix(updates)
    
    return sparseUpdate

def smoothIndex(source,index,streamlineCount,neighborhood,sulcalDepth,kernelWeight,
                normalize=False):
    
    """
    Smooth streamline counts for single index.
    
    Parameters:
    - - - - -
        source : row index
        index : target index
        streamlineCount : number of streamlines projecting to index
        neighborhood : neighboring vertices and their distances from index
        sulcalDepth : depth of index
        kernelWeight : denominator of Gaussian kernel
        
    """

    # update neighborhood with current index
    neighborhood.update({index: 0})
    
    # get distances for neighbors and index (should be 0)
    distances = np.asarray(neighborhood.values())
    # compute weights
    weights = np.exp((1. * distances) / (1. * kernelWeight * sulcalDepth))
    
    # update counts, based on weights
    # if normalize, divide by weight sum to maintain aggregate streamline counts
    if normalize:
        updatedCounts = (1. * streamlineCount * weights) / np.sum(weights)
    else:
        updatedCounts = (1. * streamlineCount * weights)
    
    updatedCounts = dict(zip(neighborhood.keys(),updatedCounts))

    return updatedCounts

def loadFDT(fdt):
    
    """
    Load fdt_matrix${number} file.
    """
    
    data = pd.read_csv(fdt,sep='\s+')
    data.columns = ['source','target','count']
    
    return data

def loadSurfaceDists(dist):
    
    """
    Load surface distance file.
    """
    
    with open(dist,'r') as inD:
        D = json.load(inD)
    
    adj = {int(k): {int(j): D[k][j] for j in D[k].keys()} for k in D.keys()}
    
    return adj
    